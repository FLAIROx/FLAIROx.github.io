@Article{Sutton88td,
	author="Sutton, Richard S.",
	title="Learning to predict by the methods of temporal differences",
	journal="Machine Learning",
	year="1988",
	month="Aug",
	day="01",
	volume="3",
	number="1",
	pages="9--44",
	abstract="This article introduces a class of incremental learning procedures specialized for prediction-that is, for using past experience with an incompletely known system to predict its future behavior. Whereas conventional prediction-learning methods assign credit by means of the difference between predicted and actual outcomes, the new methods assign credit by means of the difference between temporally successive predictions. Although such temporal-difference methods have been used in Samuel's checker player, Holland's bucket brigade, and the author's Adaptive Heuristic Critic, they have remained poorly understood. Here we prove their convergence and optimality for special cases and relate them to supervised-learning methods. For most real-world prediction problems, temporal-difference methods require less memory and less peak computation than conventional methods and they produce more accurate predictions. We argue that most problems to which supervised learning is currently applied are really prediction problems of the sort to which temporal-difference methods can be applied to advantage.",
	issn="1573-0565",
	doi="10.1007/BF00115009",
}

@book{Sutton18,
	added-at = {2019-07-13T10:11:53.000+0200},
	author = {Sutton, Richard S. and Barto, Andrew G.},
	biburl = {https://www.bibsonomy.org/bibtex/2f46601cf8b13d39d1378af0d79438b12/lanteunis},
	edition = {Second},
	publisher = {The MIT Press},
	timestamp = {2019-07-13T10:11:53.000+0200},
	title = {Reinforcement Learning: An Introduction},
	url = {http://incompleteideas.net/book/the-book-2nd.html},
	year = {2018 }
}
@inproceedings{Baird95,
	abstract = {A number of reinforcement learning algorithms have been developed that are guaranteed to converge to the optimal solution when used with lookup tables. It is shown, however, that these algorithms can easily become unstable when implemented directly with a general function-approximation system, such as a sigmoidal multilayer perceptron, a radial-basisfunction system, a memory-based learning system, or even a linear function-approximation system. A new class of algorithms, residual gradient algorithms, is proposed, which perform gradient descent on the mean squared Bellman residual, guaranteeing convergence. It is shown, however, that they may learn very slowly in some cases. A larger class of algorithms, residual algorithms, is proposed that has the guaranteed convergence of the residual gradient algorithms, yet can retain the fast learning speed of direct algorithms. In fact, both direct and residual gradient algorithms are shown to be special cases of residual algorithms, and it is shown that residual algorithms can combine the advantages of each approach. The direct, residual gradient, and residual forms of value iteration, Qlearning, and advantage learning are all presented. Theoretical analysis is given explaining the properties these algorithms have, and simulation results are given that demonstrate these properties.},
	author = {Baird, Leemon},
	doi = {10.1.1.48.3256},
	issn = {00043702},
	journal = {International Conference of Machine Learning},
	pages = {30--37},
	title = {{Residual algorithms: Reinforcement learning with function approximation}},
	booktitle={Proceedings of the Twelfth International Conference on Machine Learning (ICML)},
	year = {1995}
}

@article{Tsitsiklis97,
	author={J. N. {Tsitsiklis} and B. {Van Roy}},
	journal={IEEE Transactions on Automatic Control},
	title={An analysis of temporal-difference learning with function approximation},
	year={1997},
	volume={42},
	number={5},
	pages={674-690},
	keywords={learning (artificial intelligence);function approximation;Markov processes;convergence;temporal-difference learning;cost-to-go function;infinite-horizon discounted Markov chain;linear function approximation;irreducible aperiodic Markov chain;finite state space;infinite state space;convergence;nonlinear function approximators;Function approximation;Convergence;Cost function;Algorithm design and analysis;Dynamic programming;Linear approximation;State-space methods;Approximation error;Approximation algorithms;Error analysis},
	doi={10.1109/9.580874},
	ISSN={2334-3303},
	month={May},}
	
	@inproceedings{Fellows23,
		author = {Fellows, Mattie and Smith, Matthew and Whiteson, Shimon},
		booktitle = {International Conference on Machine Learning},
		title = {Why Target Networks Stabilise Temporal Difference Methods},
		year = {2023}
	}
	
	@article{bh2018finite,
		title={A Finite Time Analysis of Temporal Difference Learning With Linear Function Approximation},
		author={Jalaj Bhandari and Daniel Russo and Raghav Singal},
		year={2018},
		eprint={1806.02450},
		journal={arXiv preprint arXiv:1806.02450},
		archivePrefix={arXiv},
		primaryClass={cs.LG}
	}
	
	@article{HuangCleanRL22,
		author  = {Shengyi Huang and Rousslan Fernand Julien Dossa and Chang Ye and Jeff Braga and Dipam Chakraborty and Kinal Mehta and JoÃ£o G.M. AraÃºjo},
		title   = {CleanRL: High-quality Single-file Implementations of Deep Reinforcement Learning Algorithms},
		journal = {Journal of Machine Learning Research},
		year    = {2022},
		volume  = {23},
		number  = {274},
		pages   = {1--18},
		url     = {http://jmlr.org/papers/v23/21-1342.html}
	}